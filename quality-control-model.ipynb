{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fa255ae-63bf-4243-85eb-fca31b8ad627",
   "metadata": {},
   "source": [
    "# Using AI Model to detect damaged packages in an in-line quality inspection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c85b77a-e960-42ea-9381-00f85b2ff88c",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bb7001",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77eb481",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0fa96c-f296-4ea6-a242-afb7b011c9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Defining the directories of the data\n",
    "root_dir = os.path.join(os.getcwd(),\"data\")\n",
    "intact_side_dir = os.path.join(root_dir, 'intact/side/')\n",
    "intact_top_dir = os.path.join(root_dir, 'intact/top/')\n",
    "damaged_side_dir = os.path.join(root_dir, 'damaged/side/')\n",
    "damaged_top_dir = os.path.join(root_dir, 'damaged/top/')\n",
    "\n",
    "\n",
    "# Load Images Function\n",
    "def load_images(dir):\n",
    "    images = []\n",
    "    for file in os.listdir(dir):\n",
    "        if file.endswith('.jpg') or file.endswith('.png'):\n",
    "            img_path = os.path.join(dir, file)\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is not None:\n",
    "                images.append(img)\n",
    "    return images\n",
    "\n",
    "# Load the images from different categories\n",
    "damaged_top_images = load_images(damaged_top_dir)\n",
    "intact_side_images = load_images(intact_side_dir)\n",
    "damaged_side_images = load_images(damaged_side_dir)\n",
    "intact_top_images = load_images(intact_top_dir)\n",
    "\n",
    "# Creating labels for the damaged and intact images\n",
    "damaged_labels = np.ones(len(damaged_side_images) + len(damaged_top_images))\n",
    "intact_labels = np.zeros(len(intact_side_images) + len(intact_top_images))\n",
    "\n",
    "# Concatenating the images and labels\n",
    "image_data = np.concatenate((damaged_side_images, intact_side_images, intact_top_images, damaged_top_images))\n",
    "image_labels = np.concatenate((damaged_labels, intact_labels))\n",
    "\n",
    "# Making sure that the data is loaded correctly by printing the shape of the loaded data\n",
    "print(\"Shape of images:\", image_data.shape)\n",
    "print(\"Shape of labels:\", image_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9660b6c0-0c1c-41a9-a78f-8213efcef957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating an array of indices for the data to be shuffled\n",
    "indices = np.arange(len(image_data))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# Use the shuffled indices to shuffle the data and labels arrays\n",
    "image_data = image_data[indices]\n",
    "image_labels = image_labels[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bae64f",
   "metadata": {},
   "source": [
    "By shuffling the data and labels arrays in this way, it is ensured that the model sees a diverse range of samples during each training phase and avoids learning patterns based on the order of the samples in the dataset. This can lead to a more robust and generalizable model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882d04b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Split the data into training, validation, and testing sets\n",
    "num_samples = len(image_data)\n",
    "num_train = int(num_samples * 0.7)\n",
    "num_val = int(num_samples * 0.15)\n",
    "num_test = num_samples - num_train - num_val\n",
    "\n",
    "train_data = image_data[:num_train]\n",
    "train_labels = image_labels[:num_train]\n",
    "\n",
    "val_data = image_data[num_train:num_train+num_val]\n",
    "val_labels = image_labels[num_train:num_train+num_val]\n",
    "\n",
    "test_data = image_data[num_train+num_val:]\n",
    "test_labels = image_labels[num_train+num_val:]\n",
    "\n",
    "print(\"Number of training samples:\", len(train_data))\n",
    "print(\"Number of validation samples:\", len(val_data))\n",
    "print(\"Number of testing samples:\", len(test_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf1a20a",
   "metadata": {},
   "source": [
    "In the above code snippet the data is divided into three sets training, validation, and testing.\n",
    "\n",
    "**Training set:**\n",
    "-  The part of the dataset used to train the machine learning model.\n",
    "\n",
    "**Validation Set:**\n",
    "- It is used during the training process to evaluate the performance of the model and fine-tune hyperparameters.\n",
    "- It helps in monitoring the model's performance on unseen data and detecting overfitting or underfitting.\n",
    "- The validation set influences decisions about the model architecture, such as choosing the number of layers, units, or dropout rates.\n",
    "- Typically, the validation set is used multiple times during the training process, and adjustments to the model are made based on its performance on this set.\n",
    "\n",
    "**Test Set:**\n",
    "- Used only once after the model has been trained and validated.\n",
    "- It provides an unbiased valuation of the model's performance on unseen data, serving as a final check for the model.\n",
    "- The test set should be completely separate from the training and validation sets, ensuring that the model's performance is evaluated on data that it has never seen before.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf676b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating and printing the distribution of labels indicating whether the packages are damaged or intact in the training, validation, and testing sets\n",
    "\n",
    "unique_labels, label_counts = np.unique(train_labels, return_counts=True)\n",
    "print(\"training:\", (dict(zip(unique_labels, label_counts))))\n",
    "\n",
    "unique_labels, label_counts = np.unique(val_labels, return_counts=True)\n",
    "print(\"validation:\",(dict(zip(unique_labels, label_counts))))\n",
    "\n",
    "unique_labels, label_counts = np.unique(test_labels, return_counts=True)\n",
    "print(\"testing:\", (dict(zip(unique_labels, label_counts))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adcf42e",
   "metadata": {},
   "source": [
    "\n",
    "> If the sets is not balanced between intact (0.0) and damaged (1.0), the data is shuffeled again by the snippet code showed before for the model to be effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd2e990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Architecture Definition\n",
    "\n",
    "nn = Sequential()\n",
    "nn.add(Conv2D(filters=64, kernel_size=3, activation='relu', input_shape=(540, 960, 3)))\n",
    "nn.add(MaxPooling2D(pool_size=2))\n",
    "nn.add(Conv2D(filters=128, kernel_size=3, activation='relu'))\n",
    "nn.add(MaxPooling2D(pool_size=2))\n",
    "nn.add(Flatten())\n",
    "nn.add(Dense(64, activation='relu'))\n",
    "nn.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compiling the Model\n",
    "nn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Printing a summary of the model architecture, including the number of parameters in each layer and the total number of trainable parameters.\n",
    "nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce104d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the Model\n",
    "\n",
    "history = nn.fit(train_data, train_labels, batch_size=32, epochs=10, validation_data=(val_data, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b416da0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = nn.evaluate(test_data, test_labels)\n",
    "print(f\"Test loss: \",test_loss)\n",
    "print(f\"Test accuracy: \",test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71369095",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
